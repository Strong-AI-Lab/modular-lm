
metric: "accuracy"
routing_weight: 0.1
mutual_information_weight: 0.5
invariant_prediction_weight: 0.5
domain_prediction_weight: 0.5
training_arguments:
  output_dir: "fine-tuning-output"
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  num_train_epochs: 3
  learning_rate: 0.0005
  report_to: "wandb"
  logging_steps: 500
peft:
  peft_type: "LORA"
  task_type: "CAUSAL_LM"
  r: 8
  lora_alpha: 16
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
  lora_dropout: 0.05
  bias: "none"