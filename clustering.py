
import numpy as np
import pandas as pd
import argparse
import yaml
import tqdm
import os
import datetime

import matplotlib.pyplot as plt

from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling
from datasets import load_dataset

from sklearn.cluster import BisectingKMeans, KMeans
from sklearn.manifold import MDS
import joblib


CLUSTERING_ALGORITHMS = {
    "Bisecting K-Means": BisectingKMeans,
    "K-Means": KMeans,
}


def main():
    parser = argparse.ArgumentParser(
        description="Cluster the embeddings of the samples from the specified dataset generated by the specified model."
    )
    parser.add_argument("model_config")
    parser.add_argument("dataset_config")
    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for the model.')
    parser.add_argument('--saved_clusters_path', type=str, default=None, help='Path to the saved clusters. If specified, the embeddings will not be recomputed.')
    args = parser.parse_args()
    
    # Load model config file
    with open(args.model_config, "r") as model_config_file:
        model_config = yaml.safe_load(model_config_file)

    # Load dataset config file
    with open(args.dataset_config, "r") as data_config_file:
        data_config = yaml.safe_load(data_config_file)


    # Load the dataset
    dataset = load_dataset(data_config["dataset_path"], **data_config["dataset_config"])
    dataset = dataset.train_test_split(test_size=0.8, shuffle=True, seed=42)
    dataset = dataset["train"]


    # Load the tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_config["model_path"], **model_config["tokenizer_config"])
    tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(model_config["model_path"], **model_config["model_config"])
    model.eval()


    # Compute the embeddings for each sample in the dataset using the loaded model
    embeddings = []
    batch = []
    for i in tqdm.trange(len(dataset)):
        text = " ".join(dataset[i]['answers']['text'])
        text = text[:1024]
        batch.append(text)

        if len(batch) == args.batch_size or i == len(dataset) - 1:
            input_ids = tokenizer(batch, return_tensors='pt', padding=True)
            batch = []

            outputs = model(**input_ids, output_hidden_states=True)
            outputs = outputs.hidden_states[-1].detach()
            outputs = outputs.sum(dim=1).numpy()

            if np.isnan(outputs).any():
                print("NaN encountered in the embeddings. Skipping this batch. This can happen when the batch size is too large.")
            else:
                embeddings.extend(outputs)

    embeddings = np.array(embeddings)


    # Run the sklearn k-means algorithm on the embeddings
    n_mds = 2
    cluster_lists = [4, 8, 16]

    project = MDS(n_components=n_mds)
    projection = project.fit_transform(embeddings)

    fig, axs = plt.subplots(
        2 * len(CLUSTERING_ALGORITHMS), len(cluster_lists), figsize=(24, 20)
    )
    axs = axs.T
    algos = []
    for k, data in enumerate([projection, embeddings]):
        for i, (algorithm_name, Algorithm) in enumerate(CLUSTERING_ALGORITHMS.items()):
            for j, n_clusters in enumerate(cluster_lists):
                        if args.saved_clusters_path is None:
                            algo = Algorithm(n_clusters=n_clusters, n_init=3)
                            algo.fit(data)
                            algos.append((algo, algorithm_name, n_clusters, n_mds if k == 0 else 0))
                            labels = algo.labels_
                        else:
                            algo = joblib.load(f"{args.saved_clusters_path}/{algorithm_name}_{n_clusters}_mds={n_mds if k == 0 else 0}.sav")
                            labels = algo.predict(data)

                        centers = algo.cluster_centers_
                        axs[j, i + k * len(CLUSTERING_ALGORITHMS)].scatter(projection[:, 0], projection[:, 1], s=10, c=labels)
                        axs[j, i + k * len(CLUSTERING_ALGORITHMS)].scatter(centers[:, 0], centers[:, 1], c="r", s=20)
                        axs[j, i + k * len(CLUSTERING_ALGORITHMS)].set_title(f"{algorithm_name} : {n_clusters} clusters {'(MDS={})'.format(n_mds) if k == 0 else ''}")


    # Save clustering results
    save_folder = f"cluster_results/{model_config['model_name']}_{data_config['dataset_name']}"
    save_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    os.makedirs(save_folder, exist_ok=True)
    plt.savefig(f"{save_folder}/{save_time}.png")


    # Save clustering models
    if args.saved_clusters_path is None:
        os.makedirs(f"{save_folder}/{save_time}", exist_ok=True)
        for algo, name, n_clusters, n_mds in algos:
            joblib.dump(algo, f"{save_folder}/{save_time}/{name}_{n_clusters}_mds={n_mds}.sav")




if __name__ == "__main__":
    main()