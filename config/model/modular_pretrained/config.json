{
    "_name_or_path": "modular-causal",
    "auto_map": {
      "AutoModel": "src.model.modular.ModularModel, ",
      "AutoConfig": "src.model.modular.ModularConfig"
    },
    "bos_token_id": 1,
    "eos_token_id": 2,
    "hidden_act": "silu",
    "hidden_size": 4096,
    "initializer_range": 0.02,
    "intermediate_size": 11008,
    "max_position_embeddings": 2048,
    "num_attention_heads": 32,
    "num_hidden_layers": 32,
    "pad_token_id": 0,
    "rms_norm_eps": 1e-06,
    "tie_word_embeddings": false,
    "torch_dtype": "float32",
    "transformers_version": "4.35.2",
    "use_cache": true,
    "vocab_size": 32000,
    "attention_bias": false,
    "num_key_value_heads": 32,
    "pretraining_tp": 1,
    "rope_scaling": null,
    "rope_theta": 10000.0,
    "base_model_config": {
      "load_in_4bit": true
    },
    "base_model_path": "/data/shared/llama2/llama-2-7b-chat-weights/",
    "hidden_dropout_prob": 0.1,
    "invariant_weight": 1.0,
    "is_peft": true,
    "mi_dim_reduction": 1024,
    "nb_modules": 2,
    "peft_config": {
      "bias": "none",
      "lora_alpha": 16,
      "lora_dropout": 0.05,
      "peft_type": "LORA",
      "r": 8,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj"
      ],
      "task_type": "CAUSAL_LM"
    },
    "routing_strategy_config": {
      "embedding_dim": 4096,
      "num_embeddings": 2,
      "quantize_fn": "hard"
    },
    "routing_strategy_name": "InputQuantizer"
  }
  